{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: this is the same code as tree.py\n",
    "\n",
    "They are the same exact code, this is just here in case someone prefers using a Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removes warnings that occassionally show in imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Libraries for website scraping\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################### Configure me! ####################\n",
    "# NOTE: if you add a website, there needs to be a function to find members' names and locations\n",
    "websites = [\n",
    "            (\"Kennet\", \"https://www.kennet.com/who-we-are/\"),\n",
    "            (\"Lullabot\", \"https://www.lullabot.com/about\"),\n",
    "            (\"Hanno\", \"https://hanno.co/team/\")\n",
    "           ]\n",
    "\n",
    "misc_is_a_category = False\n",
    "\n",
    "# Each location is stored as a list because some locations can go by multipe names\n",
    "desired_locations = list()\n",
    "if misc_is_a_category:\n",
    "    desired_locations.append([\"Silicon Valley\", \"Bay Area\", \"San Francisco\", \"Mountain View\", \"Oakland, CA\"])\n",
    "    desired_locations.append([\"London\", \"London, United Kingdom\"])\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function is only used if the user specifies their own locations (when misc_is_a_category == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categorize(location, locations):\n",
    "    for i in range(len(locations)):\n",
    "        if location in locations[i]:\n",
    "            return locations[i][0]\n",
    "    return \"Misc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function isn't necessary to understand.  It just formats the website data into something the D3 visualization tree can read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def organize(location_to_person_map):\n",
    "    ret = []\n",
    "    for location in location_to_person_map.keys():\n",
    "        children = []\n",
    "        for child in location_to_person_map[location]:\n",
    "            children.append({\"name\": child})\n",
    "        ret.append({ \"name\": location, \"children\": children })\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If more websites are desired, all that's needed are two functions that can point to each members' name and location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_locations_Kennet(page):\n",
    "    return page.findAll(\"span\", {\"class\": \"location\"})\n",
    "\n",
    "def find_name_Kennet(page_location):\n",
    "    return page_location.parent.find(\"img\")['alt']\n",
    "\n",
    "def find_locations_Lullabot(page):\n",
    "    locations = page.findAll(\"div\", {\"class\": \"layout-who-we-are__staff__member-hover__location\"})\n",
    "    return locations\n",
    "\n",
    "def find_name_Lullabot(page_location):\n",
    "    return page_location.parent.parent.find(\"div\", {\"class\": \"headshot-square__title\"}).text\n",
    "  \n",
    "def find_locations_Hanno(page):\n",
    "    info = page.findAll(\"div\", {\"class\": \"card__image-overlay__content\"})\n",
    "    locations = [elem.find(\"p\") for elem in info]\n",
    "    return locations\n",
    "\n",
    "def find_name_Hanno(page_location):\n",
    "    return page_location.parent.find(\"h3\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once the unique finder is made for a website, the function names should be added here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_finders(website_name):\n",
    "    if(website_name == \"Kennet\"):\n",
    "        return find_locations_Kennet, find_name_Kennet\n",
    "    elif(website_name == \"Lullabot\"):\n",
    "        return find_locations_Lullabot, find_name_Lullabot\n",
    "    elif(website_name == \"Hanno\"):\n",
    "        return find_locations_Hanno, find_name_Hanno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is a helper function for the main function \"scrape\".  \n",
    "\n",
    "This function returns an organized map of members' names and locations for one particular website.  It needs to be fed the parameter \"page_locations\" which is a list of every member's location.  A member's name and location are found because these are generally placed in the same HTML div and \"page_locations\" is a list of pointers that target near this div."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_location_person_map(page_locations, name_finder, misc_is_a_category, desired_locations):\n",
    "    location_to_person_map = {}\n",
    "    for page_location in page_locations:\n",
    "        member_name =  name_finder(page_location)\n",
    "        page_location = page_location.text\n",
    "        \n",
    "        if(misc_is_a_category):\n",
    "            member_location = categorize(page_location, desired_locations)\n",
    "        else:\n",
    "            member_location = page_location \n",
    "        \n",
    "        if(member_location not in location_to_person_map):\n",
    "            location_to_person_map[member_location] = []\n",
    "            \n",
    "        location_to_person_map[member_location].append(member_name)\n",
    "        \n",
    "    return organize(location_to_person_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is the main function.  It loops through each website, pulls its data, and then orgainizes it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape(websites, misc_is_a_category, desired_locations):\n",
    "    \n",
    "    # It looks strange, but the data needs to be stored in this way for the visualization tree to read it properly\n",
    "    master_json = {\"name\": \"Investors\", \"children\" :[]}\n",
    "    for i in range(len(websites)):\n",
    "        print(\"Fetching data from website #{} out of {} total webites\".format(str(i + 1), str(len(websites))))\n",
    "        website = websites[i]\n",
    "        location_finder, name_finder = get_finders(website[0])\n",
    "\n",
    "        # Retrieve the website data from the url\n",
    "        page = urllib.request.urlopen(website[1])\n",
    "        page = BeautifulSoup(page)\n",
    "\n",
    "        # Get all the profiles' locations\n",
    "        page_locations = location_finder(page)\n",
    "\n",
    "        # Sets up a map that lists the names with the locations\n",
    "        location_to_person_map = get_location_person_map(page_locations, name_finder, misc_is_a_category, desired_locations)\n",
    "        \n",
    "        ret = {\"name\": website[0], \"children\": location_to_person_map}\n",
    "        master_json[\"children\"].append(ret)\n",
    "    return master_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This hash contains all of the webpages relevant information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from website #1 out of 3 total webites\n",
      "Fetching data from website #2 out of 3 total webites\n",
      "Fetching data from website #3 out of 3 total webites\n"
     ]
    }
   ],
   "source": [
    "master_json = scrape(websites, misc_is_a_category, desired_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saves everything to a json file so the tree will work properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('tree_files/investors.json', 'w') as scraped_data_file:\n",
    "    json.dump(master_json, scraped_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opens the D3 tree html page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "import os\n",
    "webbrowser.open('file://' + os.path.realpath('tree_files/tree.html'), new=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
